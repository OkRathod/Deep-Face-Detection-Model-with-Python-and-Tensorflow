{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32780f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef11097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall labelImg tensorflow opencv-python matplotlib albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6766dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import uuid\n",
    "import cv2 as cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH=os.path.join('data','images')\n",
    "number_of_images = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341eba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "for imgnum in range(number_of_images):\n",
    "    print('Collecting images{}'.format(imgnum))\n",
    "    ret , frame = cap.read()\n",
    "    imgname=os.path.join(IMAGES_PATH,f'{str(uuid.uuid1())}.jpg')\n",
    "    cv.imwrite(imgname,frame)\n",
    "    cv.imshow('Frame',frame)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07c74d",
   "metadata": {},
   "source": [
    "### Annotate with LabelImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c03d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26dd71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.data.Dataset.list_files('data\\\\images\\\\*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c63d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(x):\n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62febef",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.map(load_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce354ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_images = image_gen.next()\n",
    "fig,ax = plt.subplots(ncols= 4, figsize=(20,20))\n",
    "for idx,image in enumerate(plt_images):\n",
    "    ax[idx].imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6641d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Data Manually into train, test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['train','test','val']:\n",
    "    for file in os.listdir(os.path.join('data',folder,'images')):\n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join('data','labels',filename)\n",
    "        if os.path.exists(existing_filepath):\n",
    "            newfile_path = os.path.join('data',folder,'labels',filename)\n",
    "            os.replace(existing_filepath,newfile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(os.path.join('data','train','images','1eba0ce5-4843-11f0-a20e-dbf8433c50c5.jpg'))\n",
    "plt.imshow(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ad8e2",
   "metadata": {},
   "source": [
    "## Apply Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=450,height=450),\n",
    "                         alb.HorizontalFlip(p=0.5),\n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2),\n",
    "                         alb.RGBShift(p=0.2),\n",
    "                         alb.VerticalFlip(p=0.5)\n",
    "                         ],\n",
    "                         bbox_params=alb.BboxParams(format='albumentations',label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(os.path.join('data','train','images','1eba0ce5-4843-11f0-a20e-dbf8433c50c5.jpg'))\n",
    "with open(os.path.join('data','train','labels','1eba0ce5-4843-11f0-a20e-dbf8433c50c5.json'),'r') as f:\n",
    "    label = json.load(f)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2712af",
   "metadata": {},
   "source": [
    "Great ‚Äî you‚Äôre working with **Create ML JSON annotation format** (used by Apple‚Äôs [Create ML](https://developer.apple.com/machine-learning/create-ml/)), which is different from formats like COCO or YOLO.\n",
    "\n",
    "Let me break it down clearly:\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Sample Annotation:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"image\": \"1eba0ce5-4843-11f0-a20e-dbf8433c50c5.jpg\",\n",
    "  \"annotations\": [\n",
    "    {\n",
    "      \"label\": \"face\",\n",
    "      \"coordinates\": {\n",
    "        \"x\": 251.99,\n",
    "        \"y\": 235.82,\n",
    "        \"width\": 202.0,\n",
    "        \"height\": 290.0\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What It Means:\n",
    "\n",
    "This annotation describes **1 bounding box** on the image `\"1eba0ce5-4843-11f0-a20e-dbf8433c50c5.jpg\"`.\n",
    "\n",
    "The **coordinates** use the following system:\n",
    "\n",
    "| Key      | Meaning                                              |\n",
    "| -------- | ---------------------------------------------------- |\n",
    "| `x`      | **X-center** of the bounding box (horizontal center) |\n",
    "| `y`      | **Y-center** of the bounding box (vertical center)   |\n",
    "| `width`  | Width of the bounding box                            |\n",
    "| `height` | Height of the bounding box                           |\n",
    "\n",
    "---\n",
    "\n",
    "### üñºÔ∏è Visual Representation:\n",
    "\n",
    "```\n",
    "             width\n",
    "      <------------------>\n",
    "       _________\n",
    "      |         |\n",
    "      |  BOX    |      ‚Üë\n",
    "      |         |      |\n",
    "      |_________|      | height\n",
    "             ‚Üë\n",
    "           (x, y) = center of box\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Convert to `[x_min, y_min, x_max, y_max]` (e.g., for Albumentations or COCO):\n",
    "\n",
    "```python\n",
    "x_center = 251.99\n",
    "y_center = 235.82\n",
    "width = 202.0\n",
    "height = 290.0\n",
    "\n",
    "x_min = x_center - width / 2 = 251.99 - 101 = 150.99\n",
    "y_min = y_center - height / 2 = 235.82 - 145 = 90.82\n",
    "x_max = x_center + width / 2 = 251.99 + 101 = 352.99\n",
    "y_max = y_center + height / 2 = 235.82 + 145 = 380.82\n",
    "```\n",
    "\n",
    "‚û°Ô∏è Final bounding box:\n",
    "\n",
    "```python\n",
    "[x_min, y_min, x_max, y_max] = [150.99, 90.82, 352.99, 380.82]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Summary:\n",
    "\n",
    "| Format Key | Description         | Example |\n",
    "| ---------- | ------------------- | ------- |\n",
    "| `x`        | Center X of the box | 251.99  |\n",
    "| `y`        | Center Y of the box | 235.82  |\n",
    "| `width`    | Box width           | 202.0   |\n",
    "| `height`   | Box height          | 290.0   |\n",
    "\n",
    "To use it with tools like Albumentations, you‚Äôll likely need to convert it to `[x_min, y_min, x_max, y_max]`.\n",
    "\n",
    "Would you like a ready-made function to do this conversion for an entire CreateML-style JSON list?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label[0]['annotations'][0]['coordinates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31329ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [0,0,0,0]\n",
    "coords[0] = label[0]['annotations'][0]['coordinates']['x'] - label[0]['annotations'][0]['coordinates']['width'] / 2\n",
    "coords[1] = label[0]['annotations'][0]['coordinates']['y'] - label[0]['annotations'][0]['coordinates']['height'] /2\n",
    "coords[2] = label[0]['annotations'][0]['coordinates']['x'] + label[0]['annotations'][0]['coordinates']['width'] / 2\n",
    "coords[3] = label[0]['annotations'][0]['coordinates']['y'] + label[0]['annotations'][0]['coordinates']['height'] /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882bfae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = list(np.divide(coords,[640,480,640,480])) # [width,height,width,height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1895d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0db5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = augmentor(image=img,bboxes=[coords],class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cad5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.rectangle(augmented['image'],\n",
    "            tuple(np.multiply(augmented['bboxes'][0][:2],[450,450]).astype(int)),\n",
    "            tuple(np.multiply(augmented['bboxes'][0][2:],[450,450]).astype(int)),\n",
    "            (255,0,0),2)\n",
    "\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65fd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in ['train','test','val']:\n",
    "    for image in os.listdir(os.path.join('data',partition,'images')):\n",
    "        img = cv.imread(os.path.join('data',partition,'images',image))\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "\n",
    "        label_path = os.path.join('data',partition,'labels',f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path,'r') as f:\n",
    "                label = json.load(f)\n",
    "            coords[0] = label[0]['annotations'][0]['coordinates']['x'] - label[0]['annotations'][0]['coordinates']['width'] / 2\n",
    "            coords[1] = label[0]['annotations'][0]['coordinates']['y'] - label[0]['annotations'][0]['coordinates']['height'] /2\n",
    "            coords[2] = label[0]['annotations'][0]['coordinates']['x'] + label[0]['annotations'][0]['coordinates']['width'] / 2\n",
    "            coords[3] = label[0]['annotations'][0]['coordinates']['y'] + label[0]['annotations'][0]['coordinates']['height'] /2\n",
    "            coords = list(np.divide(coords,[640,480,640,480]))\n",
    "        \n",
    "        try:\n",
    "            for x in range(60):\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                cv.imwrite(os.path.join('aug_data',partition,'images',f'{image.split(\".\")[0]}.{x}.jpg'),augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0:\n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0\n",
    "                    else:\n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else:\n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0\n",
    "                \n",
    "                with open(os.path.join('aug_data',partition,'labels',f'{image.split(\".\")[0]}.{x}.json'),'w') as f:\n",
    "                    json.dump(annotation,f)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data\\\\train\\\\images\\\\*.jpg',shuffle=False)\n",
    "train_images = train_images.map(load_files)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x,(120,120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8180cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data\\\\test\\\\images\\\\*.jpg',shuffle=False)\n",
    "test_images = test_images.map(load_files)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x,(120,120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2bec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data\\\\val\\\\images\\\\*.jpg',shuffle=False)\n",
    "val_images = val_images.map(load_files)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x,(120,120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf26bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(),'r',encoding=\"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "    return [label['class']],label['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63300b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_label(x):\n",
    "    class_label, bbox = tf.py_function(load_labels, [x], [tf.uint8, tf.float16])\n",
    "    class_label.set_shape([1])    # shape must be defined\n",
    "    bbox.set_shape([4])           # shape must be defined\n",
    "    return class_label, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ede98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle=False)\n",
    "train_labels = train_labels.map(parse_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle=False)\n",
    "test_labels = test_labels.map(parse_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle=False)\n",
    "val_labels = val_labels.map(parse_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d20ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in train.take(1):\n",
    "    print(\"Image shape:\", img.shape)\n",
    "    print(\"Label[0] shape:\", label[0].shape)\n",
    "    print(\"Label[1] shape:\", label[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce7a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle=False)\n",
    "# train_labels = train_labels.map(lambda x: tf.py_function(load_labels,[x],[tf.uint8,tf.float16])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle=False)\n",
    "# test_labels = test_labels.map(lambda x: tf.py_function(load_labels,[x],[tf.uint8,tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle=False)\n",
    "# val_labels = val_labels.map(lambda x: tf.py_function(load_labels,[x],[tf.uint8,tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images),len(train_labels),len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images,train_labels))\n",
    "train = train.shuffle(5000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4542328",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images,test_labels))\n",
    "test = test.shuffle(5000)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c605da",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images,val_labels))\n",
    "val = val.shuffle(5000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3388c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc0a4fc",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "error                                     Traceback (most recent call last)\n",
    "Cell In[381], line 5\n",
    "      3 sample_image = res[0][idx]\n",
    "      4 sample_coords = res[1][1][idx]\n",
    "----> 5 cv.rectangle(sample_image,\n",
    "      6              tuple(np.multiply(sample_coords[:2],[120,120]).astype(int)),\n",
    "      7              tuple(np.multiply(sample_coords[2:],[120,120]).astype(int)),\n",
    "      8              (255,0,0),2\n",
    "      9              )\n",
    "     10 ax[idx].imshow(sample_image)\n",
    "\n",
    "error: OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'rectangle'\n",
    "> Overload resolution failed:\n",
    ">  - img marked as output argument, but provided NumPy array marked as readonly\n",
    ">  - img marked as output argument, but provided NumPy array marked as readonly\n",
    ">  - Expected Ptr<cv::UMat> for argument 'img'\n",
    ">  - Expected Ptr<cv::UMat> for argument 'img'\n",
    "\n",
    "The error you're encountering:\n",
    "\n",
    "```\n",
    "error: OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'rectangle'\n",
    "> img marked as output argument, but provided NumPy array marked as readonly\n",
    "```\n",
    "\n",
    "‚Ä¶means that OpenCV's `cv.rectangle()` is trying to modify an image (`sample_image`), but the array is **read-only** ‚Äî it was likely returned in a non-writeable format.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Solution\n",
    "\n",
    "You need to make a **writable copy** of the image before modifying it. You can do this with `.copy()`.\n",
    "\n",
    "### ‚úÖ Updated code:\n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "\n",
    "for idx in range(4):\n",
    "    sample_image = res[0][idx].copy()  # Make a writable copy\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "    # Convert normalized coordinates to image scale\n",
    "    top_left = tuple(np.multiply(sample_coords[:2], [120, 120]).astype(int))\n",
    "    bottom_right = tuple(np.multiply(sample_coords[2:], [120, 120]).astype(int))\n",
    "    \n",
    "    # Draw rectangle\n",
    "    cv.rectangle(sample_image, top_left, bottom_right, (255, 0, 0), 2)\n",
    "    \n",
    "    # Show the image\n",
    "    ax[idx].imshow(sample_image)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Optional Improvements:\n",
    "\n",
    "* Make sure `[120, 120]` matches the actual size of your image if not fixed.\n",
    "* Consider adding `ax[idx].axis('off')` to hide axis labels.\n",
    "\n",
    "Let me know if you want help scaling coordinates or displaying more images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=4,figsize=(20,20))\n",
    "for idx in range(4):\n",
    "    sample_image = res[0][idx].copy()\n",
    "    sample_coords = res[1][1][idx]\n",
    "    cv.rectangle(sample_image,\n",
    "                 tuple(np.multiply(sample_coords[:2],[120,120]).astype(int)),\n",
    "                 tuple(np.multiply(sample_coords[2:],[120,120]).astype(int)),\n",
    "                 (255,0,0),2\n",
    "                 )\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e7cd45",
   "metadata": {},
   "source": [
    "## Deep Learning Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fad93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdd02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed4666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_layer = Input(shape=(120,120,3))\n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "    \n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1,activation='sigmoid')(class1)\n",
    "\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4,activation='sigmoid')(regress1)\n",
    "\n",
    "    facetracker = Model(inputs= input_layer, outputs=[class2,regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4645ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0186fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = train.as_numpy_iterator().next()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6af04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords= facetracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33035141",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3819a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1./0.75 - 1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33484556",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt =tf.keras.optimizers.Adam(learning_rate=0.0001,decay = lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):\n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2]- yhat[:,:2]))\n",
    "\n",
    "    h_true = y_true[:,3] - y_true[:,1]\n",
    "    w_true = y_true[:,2] - y_true[:,0]\n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1]\n",
    "    w_pred = yhat[:,2] - yhat[:,0]\n",
    "\n",
    "    delta_size = tf.reduce_sum(tf.square( w_true - w_pred ) + tf.square( h_true - h_pred))\n",
    "    return delta_coord + delta_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07756e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1],coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0],classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3863f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressloss(y[1],coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57074609",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.as_numpy_iterator().next()\n",
    "tf.print(\"Type of y:\", type(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e20a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model): \n",
    "    def __init__(self, eyetracker,  **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.model = eyetracker\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "    \n",
    "    def train_step(self, batch, **kwargs): \n",
    "        \n",
    "        X, y = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            classes, coords = self.model(X, training=True)\n",
    "            \n",
    "            batch_classloss = self.closs(tf.cast(y[0], tf.float32), classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "            \n",
    "            total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "            \n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(tf.cast(y[0], tf.float32), classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "        \n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FaceTracker(Model):\n",
    "#     def __init__(self,eyetracker,**kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.model = eyetracker\n",
    "    \n",
    "#     def complie(self,opt,classloss,regressloss,**kwargs):\n",
    "#         super().compile(**kwargs)\n",
    "#         self.closs = classloss\n",
    "#         self.lloss = regressloss\n",
    "#         self.opt = opt\n",
    "\n",
    "#     def train_step(self, batch, **kwargs):\n",
    "#         X,y = batch\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             classes , coords = self.model(X,training = True)\n",
    "#             batch_classloss = self.closs(y[0],classes)\n",
    "#             batch_localizationloss = self.lloss(tf.cast(y[1],tf.float32),coords)\n",
    "#             total_loss = batch_localizationloss + 0.5 * batch_classloss\n",
    "\n",
    "#             grad = tape.gradient(total_loss,self.model.trainable_variables)\n",
    "#         opt.apply_gradients(zip(grad,self.model.trainable_variables))\n",
    "#         return {\"Total Loss\":total_loss,\"class loss\":batch_classloss,\"regress loss\":batch_localizationloss}\n",
    "    \n",
    "#     def test_step(self, batch, **kwargs):\n",
    "#         X,y = batch\n",
    "#         classes, coords = self.model(X, training = False)\n",
    "#         batch_classloss = self.closs(y[0],classes)\n",
    "#         batch_localizationloss = self.lloss(tf.cast(y[1],tf.float32),coords)\n",
    "#         total_loss = batch_localizationloss + 0.5* batch_classloss\n",
    "\n",
    "#         return {\"Total Loss\":total_loss,\"class loss\":batch_classloss,\"regress loss\":batch_localizationloss}\n",
    "    \n",
    "#     def call(self,X,**kwargs):\n",
    "#         return self.model(X,**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6603fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2fa221",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt,classloss,regressloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6030a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29051cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train.take(1):\n",
    "    print(\"X shape:\", x.shape)\n",
    "    print(\"y[0] shape:\", y[0].shape)\n",
    "    print(\"y[1] shape:\", y[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ce063",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=40, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07931f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax= plt.subplots(ncols=3,figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'],color='teal',label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'],color='orange',label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'],color='teal',label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'],color='orange',label='val class loss')\n",
    "ax[1].title.set_text('Class Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'],color='teal',label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'],color='orange',label='val regress loss')\n",
    "ax[2].title.set_text('Regress Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b312f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_data.next()\n",
    "yhat = facetracker.predict(test_sample[0])\n",
    "fig, ax = plt.subplots(ncols=4,figsize=(20,20))\n",
    "for idx in range(4):\n",
    "    sample_image = test_sample[0][idx].copy()\n",
    "    sample_coords = yhat[1][idx]\n",
    "\n",
    "    if yhat[0][idx] > 0.5:\n",
    "        cv.rectangle(sample_image,\n",
    "                     tuple(np.multiply(sample_coords[:2],[120,120]).astype(int)),\n",
    "                     tuple(np.multiply(sample_coords[2:],[120,120]).astype(int)),\n",
    "                     (255,0,0),2\n",
    "                     )\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00638353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "facetracker.save('FaceTracker.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba41cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = load_model('FaceTracker.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _,frame = cap.read()\n",
    "    frame = frame[50:500,50:500,:]\n",
    "    rgb = cv.cvtColor(frame,cv.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb,(120,120))\n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    if yhat[0] > 0.5:\n",
    "        cv.rectangle(frame,\n",
    "                     tuple(np.multiply(sample_coords[:2],[450,450]).astype(int)),\n",
    "                     tuple(np.multiply(sample_coords[2:],[450,450]).astype(int)),\n",
    "                     (0,255,0),2\n",
    "                     )\n",
    "        \n",
    "        # cv.rectangle(frame,\n",
    "        #              tuple(np.add(np.multiply(sample_coords[:2],[450,450]).astype(int),[0,-30])),\n",
    "        #              tuple(np.add(np.multiply(sample_coords[2:],[450,450]).astype(int),[80,0])),\n",
    "        #              (255,0,0),1\n",
    "        #              )\n",
    "        \n",
    "        cv.putText(frame,\n",
    "                   'FACE',\n",
    "                   tuple(np.add(np.multiply(sample_coords[:2],[450,450]).astype(int),[0,-5])),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX,\n",
    "                   1,(255,255,255),2,cv.LINE_AA\n",
    "                   )\n",
    "    \n",
    "    cv.imshow('FaceTracker',frame)\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0904aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facedetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
